{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "[Forward Propagation](#Forward-Propagation)  \n",
    "[Padding](#Padding)  \n",
    "[Conv_Forward](#Conv_Forward)  \n",
    "[Pooling_Forward](#Pooling_Forward)  \n",
    "[Softmax_Forward](#Softmax_Forward)  \n",
    "[Backpropagation](#Backpropagation)  \n",
    "[Conv_Backward](#Conv_Backward)  \n",
    "[Pooling_Backward](#Pooling_Backward)  \n",
    "[Softmax_Backward](#Softmax_Backward)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Temporary working directory of CIFAR-10\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_structure(X, y, hidden_size):\n",
    "    '''\n",
    "    Arguments:\n",
    "    \n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero padding \n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad all images in dataset X with zeros along image height and width.\n",
    "    \n",
    "    Arguments:\n",
    "        X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch\n",
    "        of m images with height n_H, width n_W, and channels n_C\n",
    "    \n",
    "    Returns:\n",
    "        X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    X_pad = np.pad(\n",
    "        X, (\n",
    "            # Do not pad the the first dimension of X (m).\n",
    "            (0, 0),\n",
    "            # Pad the second dimension of X (n_H).\n",
    "            (pad, pad),\n",
    "            # Pad the third dimension of X (n_W).\n",
    "            (pad, pad),\n",
    "            # Do not pad the fourth dmesnion of X (n_C).\n",
    "            (0, 0)),\n",
    "        # Pad with a constant value.\n",
    "        mode = 'constant',\n",
    "        # Assign zero constant values to pad before and after each axis.\n",
    "        constant_values = (0,0))\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv_Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution single step\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation\n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "        a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "        W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "        b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "        Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the element-wise product between a_slice_prev and W and save to a variable.\n",
    "    s = np.multiply(a_slice_prev, W)\n",
    "    \n",
    "    # Sum over all the entries of the volume.\n",
    "    Z = np.sum(s)\n",
    "    \n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z = Z + np.float(b)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional layer.\n",
    "# This needs to be vectorized.\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function.\n",
    "    \n",
    "    Arguments:\n",
    "        A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "        b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "        hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- convolution output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for backward propagation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract dimensions from A_prev's shape.\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Extract dimensions from W's shape.\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Extract information from \"hparameters\" dictionary.\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume.\n",
    "    n_H = int((n_H_prev - f + 2*pad)/stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2*pad)/stride) + 1\n",
    "    \n",
    "    # Initialize the output volume Z with zeros.\n",
    "    Z = np.zeros([m, n_H, n_W, n_C])\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev using the function.\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    # Loop over the batch of training examples.\n",
    "    for i in range(m):\n",
    "        # Select the ith training example's padded activation.\n",
    "        a_prev_pad = A_prev_pad[i, :, :, :]\n",
    "        # Loop over the vertical axis of the output volume.\n",
    "        for h in range(n_H):\n",
    "            # Loop over the horizontal axis of the output volume.\n",
    "            for w in range(n_W):\n",
    "                # Loop over the channels of the output volume.\n",
    "                for c in range(n_C):\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the 3D slide of a_prev_pad.\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    # Convolve the 3D slide with the correct filter W and bias b, to get back one output neuron.\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:, :, :, c], b[:, :, :, c])\n",
    "                    \n",
    "    # Check to make sure your output shape is correct.\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information for backpropagation.\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "                    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling_Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pool layer\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "        A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "        mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "        A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "        cache -- cache used in the backward pass of the pooling layer, contains the input and hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract dimensions from the input shape.\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Extract hyperparameters from \"hparameters\".\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output.\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "              \n",
    "    # Initialize the output matrix A.\n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Loop over the training examples.\n",
    "    for i in range(m): \n",
    "        # Loop on the vertical axis of the output volume.\n",
    "        for h in range (n_H):\n",
    "            # Loop on the horizontal axis of the output volume.\n",
    "            for w in range(n_W):\n",
    "                # Loop over the channels of the output volume.\n",
    "                for c in range(n_C):\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c.\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for backward prop.\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct.\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC layer @ end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax_Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv_Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function.\n",
    "    \n",
    "    Arguments:\n",
    "        dZ -- gradient of the cost with repsect to the output of conv layer (Z), numpy array\n",
    "              of shape (m, n_H, n_W, n_C)\n",
    "        cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "        \n",
    "    Returns:\n",
    "        dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                   numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "              numpy array of shape (f, f, n_C_prev, n_C)\n",
    "        db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "              numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract information from the \"cache\".\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Extract dimensions from A_prev's shape.\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Extract dimensions from W's shape.\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\".\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape.\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, dB with the correct shapes.\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "    \n",
    "    # Pad A_prev and dA_prev\n",
    "    # This pad is stolen from zero_pad function above.\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    da_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    # Loop over training examples.\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        # Loop over vertical axes of the output volume.\n",
    "        for h in range(n_H):\n",
    "            # Loop over horizontal axis of the output volume.\n",
    "            for w in range(n_W):\n",
    "                # Loop over the channels of the output volume.\n",
    "                for c in range(n_C):\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\".\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    # Update gradients for the window and the filter's parameters.\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start, horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "        \n",
    "        # Set the ith training example's dA_prev to the upadded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "        \n",
    "    # Make sure your output shape is correc.t\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling_Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max pooling - backward pass\n",
    "# Define a helper function to create a mask matrix which keeps track of the maximum.\n",
    "def create_mask_from_window(X):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix X, to identify the max entry of X.\n",
    "    \n",
    "    Arugments:\n",
    "        X -- Array of shape (f, f)\n",
    "        \n",
    "    Returns:\n",
    "        mask -- Array with same shape as window, contains a True at the position corresponding to the max entry of X.\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = X == np.max(X)\n",
    "    \n",
    "    return mask \n",
    "\n",
    "# Average pooling - backward pass\n",
    "# Define a helper function to distribute values from average pooling evenly.\n",
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix with dimensions shape.\n",
    "    \n",
    "    Arguments:\n",
    "        dz -- input scalar\n",
    "        shape -- the shape (n_H, n_W) of output matrix for which we want to distribute the value of dz.\n",
    "        \n",
    "    Returns:\n",
    "        a -- Array of size (n_H, n_W) for which we distributed the value of dz.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract dimensions from shape.\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix.\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    # Createa a matrix where every entry is the \"average\" value.\n",
    "    a = np.ones(shape) * average\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it together in pooling backward.\n",
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer.\n",
    "    \n",
    "    Arguments:\n",
    "        dA -- gradient of cost with respect tothe output of the pooling layer, same shape as A.\n",
    "        cache -- cache output from the forward pass of the pooling layer, contains the layer's\n",
    "                 input and hparameters\n",
    "        mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "        dA_prev -- gradient of cost \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax_Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Update (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to just have a convolutional network. Have a pretty vanilla implementation of a ConvNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
