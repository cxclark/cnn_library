{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebooks\n",
    "[01_data_visualization](#01_data_visualization.ipynb)  \n",
    "[02_formulas](#02_formulas.ipynb)  \n",
    "[03_demo](03_demo.ipynb)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Notebook's Contents\n",
    "[Layers](#Layers)  \n",
    "[Scoring Function](#Scoring-Function)  \n",
    "[Convolution Output Size](#Convolution-Output-Size)  \n",
    "[Padding](#Padding)  \n",
    "[ReLU](#ReLU)  \n",
    "[Softmax Classifier](#Softmax-Classifier)  \n",
    "[Categorical Cross-Entropy Loss](#Categorical-Cross-Entropy-Loss)  \n",
    "[Backpropagation](#Backpropagation)  \n",
    "[Stochastic Gradient Descent](#Stochastic-Gradient-Descent)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "In its simplest form, a CNN classifier for CIFAR-10 might look like this:\n",
    "- Convolutionlayer: computes output of neurons connected to local regions in the input, changes output size.\n",
    "    - Has parameters: weights and biases\n",
    "    - Has hyperparameters: num_filters (K), filter_size (F), stride (S), zero-padding(P)\n",
    "- ReluLayer: applies element-wise activation function max(0, x), no change in output size.\n",
    "- PoolingLayer: downsamples along width and height, changes output size.\n",
    "    - Has hyperparameters: filter_size (F), stride (S)\n",
    "- DenseLayer: computes class scores, where numbers correspond to CIFAR-10 class score.\n",
    "    - Has parameters: weights and biases\n",
    "    - Has hyperparameter: units\n",
    "- SoftMaxLayer: applies exponentiation and normalization to the the DenseLayer output scores.\n",
    "\n",
    "The formulas below are not exhaustive. They cover the core calculations used in the library. Note that when vectorized, a variable $w$ is written as $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Function\n",
    "\n",
    "$$Z = WX + B$$\n",
    "\n",
    "Dot product layers in a neural network, such as convolution and dense layers, take some input $X$, multiply it by weights $W$, add biases $B$ to output scores $Z$.\n",
    "\n",
    "Deeper in the network, the input to a layer are the activation outputs of the previous layer. Output values $Z$ of a given layer $L$ can be expressed as:\n",
    "\n",
    "$$Z^{[L]} = W^{[L]}A^{[L-1]} + B^{[L]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Output Size\n",
    "Given an $n$ x $n$ image, $f$ x $f$ filter, padding $p$, and stride $s$, the output volume of a convolution can be expressed as:\n",
    "\n",
    "$$\\left[  \\frac{n+2p-f}{s} + 1 \\right] \\text{x} \\left[  \\frac{n+2p-f}{s} + 1 \\right] \\text{x filters}$$\n",
    "\n",
    "The depth of the output volume is equivalent to the number of filters used in the convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "Formula for padding so that output is same as input size:\n",
    "\n",
    "$$p = \\frac{f-1}{2}$$\n",
    "\n",
    "Common to use this to preserve size spatially during convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "Rectified Linear Units or ReLU are used as standard activation functions in most neural networks, including CNNs: \n",
    "\n",
    "$$f(x) = max(0,x)$$\n",
    "\n",
    "ReLU does not saturate in the positive region and is computationally efficient. It converges much faster than sigmoid/tanh in practice (e.g. 6X), but doesn't have 0-centered output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Once you get scores $Z$ out of the last Dense layer, it's up to you how to interpret them. The industry standard for interpreting classification problems with more than two classes is Softmax.\n",
    "\n",
    "Softmax interprets $Z$ scores as the unnormalized log probabilities of the classes. To convert to probabilities, exponentiate and normalize the scores. The probability for a class $k$ with score $s$ can be expressed as:\n",
    "\n",
    "$$P(Y = k|X = x_i) = \\frac{e^s_k}{\\sum_{j} e^s_j}$$\n",
    "\n",
    "You exponentiate the scores for one class, and divide by the sum of exponentiated scores for all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Cross-Entropy Loss\n",
    "\n",
    "To optimize the network, we need a loss or cost function to minimize. If we want to maximize the likelihood of the correct class, then we want to minimize the negative log likelihood of the correct class:\n",
    "\n",
    "$$L_i = -logP(Y = y_i|X = x_i)$$\n",
    "\n",
    "Log is used because it works better mathematically. Substituting in the formula above, loss for a single example can be expressed as:\n",
    "\n",
    "$$L_i = -log\\frac{e^s_{y_i}}{\\sum_{j} e^s_j}$$\n",
    "\n",
    "Loss over the entire training set can be expressed as:\n",
    "\n",
    "$$L = \\frac{1}{N}\\sum^N_{i=1}L_i + R(W)$$\n",
    "\n",
    "Regularization (e.g. L2, L1) is only a function of the weights, not the data.\n",
    "\n",
    "**Sanity check when kicking off classifier training**\n",
    "\n",
    "At the beginning of training, your weights will be small, so the scores of all classes should be close to ~0. Exponentiating 0 gives 1, and normalizing gives (1/num_classes). Thus, the *loss* when kicking things off should be $-log\\frac{1}{NumOfClasses}$. If it isn't, then something isn't set up properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "There is a calculus derivation of the loss that won't be covered here. But the result is that the key equation needed to initialize backprop, where $\\hat Y$ is a prediction vector and $Y$ is a true label vector, is:\n",
    "\n",
    "$$dZ^{[L]} = \\hat Y - Y$$\n",
    "\n",
    "$dZ^{L}$ is a partial derivative of the cost function with respect to the outputs of the last layer:\n",
    "$$\\frac{\\partial J}{\\partial Z^{[L]}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Batch gradient descent is the simplest parameter update method. For every layer $l$ with weights $W$, biases $b$, and learning rate $\\alpha$:\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$\n",
    "\n",
    "$dW$ and $db$ are the local gradients for $W$ and $b$. They are derived via the chain rule from the previous layer during backpropagation.\n",
    "\n",
    "Stochastic gradient descent, or mini-batch gradient descent, executes gradient descent over smaller batches of training data. Common mini-batch sizes are 32, 64, 128, and 256."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
