{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Function\n",
    "\n",
    "$$WX + B$$\n",
    "\n",
    "Once you get scores out, it's up to you how to interpret them and develop a loss function. You can pipe them into SVMs or Softmax, for example. But there are others.\n",
    "\n",
    "## SVM Classifier\n",
    "You can intepret the scores via SVM where you just want the correct score to be some margin above the incorrect scores, but in practice softmax is used more often--where you interpret the scores to be unnormalized log probabilities.\n",
    "\n",
    "$$L_i = \\sum_{j\\neq y_i}max(0,s_j - s_{y_i} + 1)$$\n",
    "\n",
    "SVM has a very local space which it cares about, the margin of 1 here, and beyond that it's invariant. So a loss of -100 versus -200 wouldn't matter to SVM, whereas it would still change loss for Softmax.\n",
    "\n",
    "You don't count the correct class, because you'd be inflating loss by 1 everywhere. It wouldn't impact ultimate performance, but kind of an arbitrary decision.\n",
    "\n",
    "## Softmax Classifier\n",
    "The loss scores for a softmax classifier are interpreted as the unnormalized log probabilities of the classes. The probabilities may be derived by exponentiating the scores and normalizing them. The probability for a class *k* can be represented:\n",
    "\n",
    "$$P(Y = k|X = x_i) = \\frac{e^s_k}{\\sum_{j} e^s_j}$$\n",
    "\n",
    "You exponentiate the scores for one class, and divide by the sum of exponentiated scores for all classes.\n",
    "\n",
    "Where the score *s* is a function of inputs and weights:\n",
    "\n",
    "$$s = f(x_i;W)$$\n",
    "\n",
    "We want to maximize the log likelihood, or (for a loss function) to minimize the negative log likelihood of the correct class:\n",
    "$$L_i = -logP(Y = y_i|X = x_i)$$\n",
    "\n",
    "*We want the log likelihood of the correct class to be high (we want the negative of it to be low), and the log likelihood is the softmax function of your scores*.\n",
    "\n",
    "In summary:\n",
    "$$L_i = -log\\frac{e^s_{y_i}}{\\sum_{j} e^s_j}$$\n",
    "\n",
    "*Summary*: You convert the score outputs as unnormalized log probabilities, so first you convert to probabilities, and then you want to maximize the log probability of the right classes, which gives us the loss function for softmax.\n",
    "\n",
    "#### Sanity check when kicking off classifier training\n",
    "\n",
    "As a sanity check at the beginning of your optimization, your weights will be small, so the scores of all classes should be ~0. The *loss* should be $$-log\\frac{1}{NumOfClasses}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Loss\n",
    "\n",
    "$$L = \\frac{1}{N}\\sum^N_{i=1}L_i + R(W)$$\n",
    "\n",
    "Loss over the entire training set. Regularization is only a function of the weights, not the data.\n",
    "\n",
    "## Optimization\n",
    "Imagine you have a loss landscape, and you're blindfolded, but you have an altimeter, and you're trying to get to the bottom of the valley. That altimeter is the process of optimization.\n",
    "\n",
    "#### Numerical Approximation of Graidents\n",
    "- When you implement backprop, do gradient checking\n",
    "\n",
    "#### Mini-batch Gradient Descent\n",
    "- Common mini-batch sizes are 32/54/128 examples\n",
    "- Kirzhevsky ILSVRC ConvNet used 256 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "(Before) Linear score function: $$f = Wx$$\n",
    "\n",
    "(Now) 2-layer Neural Network: $$f = W_2max(0,W_1x)$$\n",
    "\n",
    "or 3-layer Neural Network: $$f = W_3max(0,W_2max(0,W_1x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "**Sigmoid**: $$\\sigma(x) = \\frac{1}{1 + e^-x}$$\n",
    "\n",
    "**tanh**: $$tanh(x)$$\n",
    "\n",
    "**ReLU**: $$max(0,x)$$\n",
    "Does not saturate in +region. Very computationally efficient. Converges much faster than sigmoid/tanh in practice (e.g. 6X). Not 0-centered output though.  \n",
    "\n",
    "**Leaky ReLU**: $$max(0.1x, x)$$\n",
    "Will not \"die\".  \n",
    "\n",
    "**Maxout**: $$max(w^T_1x + b_1, w^T_2x + b_2)$$\n",
    "\n",
    "**ELU**: (couldn't copy formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay\n",
    "\n",
    "**step decay**:\n",
    "e.g. decay learning rate by half every few epochs\n",
    "\n",
    "**exponential decay**:\n",
    "$$\\alpha = \\alpha_0e^{-kt}$$\n",
    "\n",
    "**1/t decay**:\n",
    "$$\\alpha = \\frac{\\alpha_0}{1 + kt}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution output size\n",
    "*n* x *n* image  \n",
    "*f* x *f* filter  \n",
    "padding *p*  \n",
    "stride *s*  \n",
    "\n",
    "$$\\left[  \\frac{n+2p-f}{s} + 1 \\right] x \\left[  \\frac{n+2p-f}{s} + 1 \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "Common to use this to preserve size spatially:\n",
    "$$\\frac{F-1}{2}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
