{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebooks\n",
    "[01_data_visualization](#01_data_visualization.ipynb)  \n",
    "[02_formulas](#02_formulas.ipynb)  \n",
    "[03_demo](03_demo.ipynb)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "In its simplest form, a CNN classifier for CIFAR-10 might look like this:\n",
    "- Convolutionlayer: computes output of neurons connected to local regions in the input, changes output size.\n",
    "    - Has parameters: weights and biases\n",
    "    - Has hyperparameters: num_filters (K), filter_size (F), stride (S), zero-padding(P)\n",
    "- ReluLayer: applies element-wise activation function max(0, x), no change in output size.\n",
    "- PoolingLayer: downsamples along width and height, changes output size.\n",
    "    - Has hyperparameters: filter_size (F), stride (S)\n",
    "- DenseLayer: computes class scores, where numbers correspond to CIFAR-10 class score.\n",
    "    - Has parameters: weights and biases\n",
    "    - Has hyperparameter: units\n",
    "- SoftMaxLayer: applies exponentiation and normalization to the the DenseLayer output scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Function\n",
    "\n",
    "$$Z = WX + B$$\n",
    "\n",
    "Dot product layers in a neural network, such as convolution and dense layers, take some input X, multiply it by weights W, add biases B to output scores Z.\n",
    "\n",
    "Deeper in the network, the input to a layer are the activation outputs of the previous layer. Output values Z of a given layer L can be expressed as:\n",
    "$$Z^{[L]} = W^{[L]}A^{[L-1]} + B^{[L]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Once you get scores Z out of the last Dense layer, it's up to you how to interpret them. The industry standard for interpreting classification problems with more than two classes is Softmax.\n",
    "\n",
    "Softmax interprets Z scores as the unnormalized log probabilities of the classes. To convert to probabilities, exponentiate and normalize the scores. The probability for a class *k* with score *s* can be expressed as:\n",
    "\n",
    "$$P(Y = k|X = x_i) = \\frac{e^s_k}{\\sum_{j} e^s_j}$$\n",
    "\n",
    "You exponentiate the scores for one class, and divide by the sum of exponentiated scores for all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "To optimize the network, we need a loss or cost function to minimize. If we want to maximize the likelihood of the correct class, then we want to minimize the negative log likelihood of the correct class:\n",
    "$$L_i = -logP(Y = y_i|X = x_i)$$\n",
    "\n",
    "We want the log likelihood of the correct class to be high (we want the negative of it to be low), and the log likelihood is the softmax function of your scores. Log rather than raw values are used because it works better mathematically.\n",
    "\n",
    "If we just substitute in the probability formula from above, loss becomes:\n",
    "$$L_i = -log\\frac{e^s_{y_i}}{\\sum_{j} e^s_j}$$\n",
    "\n",
    "### Sanity check when kicking off classifier training\n",
    "\n",
    "As a sanity check at the beginning of training, your weights will be small, so the scores of all classes should be close to ~0. Exponentiating 0 gives 1, and normalizing gives (1/num_classes). Thus, the *loss* when kicking things off should be $$-log\\frac{1}{NumOfClasses}$$\n",
    "If it isn't, then something isn't set up properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Loss\n",
    "\n",
    "$$L = \\frac{1}{N}\\sum^N_{i=1}L_i + R(W)$$\n",
    "\n",
    "Loss over the entire training set. Regularization is only a function of the weights, not the data.\n",
    "\n",
    "## Backpropagation Step\n",
    "Key step or equation you need to initialize backprop is:\n",
    "\n",
    "$$dZ^{[L]} = \\hat Y - Y$$\n",
    "\n",
    "dZ is a partial derivative of the cost function with respect to the outputs of the last layer:\n",
    "$$\\frac{\\partial J}{\\partial Z^{[L]}}$$\n",
    "\n",
    "## Optimization\n",
    "Imagine you have a loss landscape, and you're blindfolded, but you have an altimeter, and you're trying to get to the bottom of the valley. That altimeter is the process of optimization.\n",
    "\n",
    "#### Numerical Approximation of Graidents\n",
    "- When you implement backprop, do gradient checking\n",
    "\n",
    "#### Mini-batch Gradient Descent\n",
    "- Common mini-batch sizes are 32/54/128 examples\n",
    "- Kirzhevsky ILSVRC ConvNet used 256 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "(Before) Linear score function: $$f = Wx$$\n",
    "\n",
    "(Now) 2-layer Neural Network: $$f = W_2max(0,W_1x)$$\n",
    "\n",
    "or 3-layer Neural Network: $$f = W_3max(0,W_2max(0,W_1x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "**Sigmoid**: $$\\sigma(x) = \\frac{1}{1 + e^-x}$$\n",
    "\n",
    "**tanh**: $$tanh(x)$$\n",
    "\n",
    "**ReLU**: $$max(0,x)$$\n",
    "Does not saturate in +region. Very computationally efficient. Converges much faster than sigmoid/tanh in practice (e.g. 6X). Not 0-centered output though.  \n",
    "\n",
    "**Leaky ReLU**: $$max(0.1x, x)$$\n",
    "Will not \"die\".  \n",
    "\n",
    "**Maxout**: $$max(w^T_1x + b_1, w^T_2x + b_2)$$\n",
    "\n",
    "**ELU**: (couldn't copy formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay\n",
    "\n",
    "**step decay**:\n",
    "e.g. decay learning rate by half every few epochs\n",
    "\n",
    "**exponential decay**:\n",
    "$$\\alpha = \\alpha_0e^{-kt}$$\n",
    "\n",
    "**1/t decay**:\n",
    "$$\\alpha = \\frac{\\alpha_0}{1 + kt}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution output size\n",
    "*n* x *n* image  \n",
    "*f* x *f* filter  \n",
    "padding *p*  \n",
    "stride *s*  \n",
    "\n",
    "$$\\left[  \\frac{n+2p-f}{s} + 1 \\right] x \\left[  \\frac{n+2p-f}{s} + 1 \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "Formula for padding so that output is same as input size. Common to use this to preserve size spatially:\n",
    "$$p = \\frac{f-1}{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
